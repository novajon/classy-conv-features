{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathanjanke/anaconda3/envs/py36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/Users/jonathanjanke/anaconda3/envs/py36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, cohen_kappa_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, KFold\n",
    "\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "\n",
    "from scipy import stats\n",
    "import glob\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "from scipy.stats import ttest_ind_from_stats\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "import copy\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"351_Data/ILSVRC2012_Validation/Intermediate/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "predefined_folder_name = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If no predefined folder name is set, the latest data is used (folder with latest timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if predefined_folder_name == \"\":\n",
    "    folder_list = np.sort(glob.glob(path + \"/*\"))\n",
    "    curr_path = folder_list[-1] + \"/\"\n",
    "else:\n",
    "    curr_path = path + predefined_folder_name + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'351_Data/ILSVRC2012_Validation/Intermediate/20181010_0850/'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = curr_path.split(\"/\")[1].split(\"_\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [fn.split(\"/\")[-1] for fn in glob.glob(curr_path + \"*\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ILSVRC2012'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Inception ResNet', 'InceptionV3', 'Xception']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = model_names[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following line is only inteded for the full model, assuming a class split of 10 classes per batch has been made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for full model\n",
    "use = False\n",
    "if use:\n",
    "    for fn in model_names:\n",
    "        train_data_parts = glob.glob(curr_path + str(fn) + \"\\\\train*\")\n",
    "        test_data_parts = glob.glob(curr_path + str(fn) + \"\\\\test*\")\n",
    "        curr_train = []\n",
    "        curr_test = []\n",
    "        for train_part, test_part in zip(train_data_parts, test_data_parts):\n",
    "            curr_train.append (np.load(train_part))\n",
    "            #print (np.load(train_part)[10][1])\n",
    "            #curr_test.append(np.load(test_part))\n",
    "\n",
    "        train_data.append(np.concatenate(curr_train))\n",
    "        #test_data.append(np.concatenate(curr_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following line is for running the model from only one data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "test_data = []\n",
    "for fn in model_names:\n",
    "    train_data_parts = np.load(glob.glob(curr_path + str(fn) + \"/train*\")[0])\n",
    "    for tt in glob.glob(curr_path + str(fn) + \"/train*\")[1:]:\n",
    "        train_data_parts = np.vstack((train_data_parts,np.load(tt)))\n",
    "    train_data.append(copy.copy(train_data_parts))\n",
    "    \n",
    "    test_data_parts = np.load(glob.glob(curr_path + str(fn) + \"/test*\")[0])\n",
    "    for tt in glob.glob(curr_path + str(fn) + \"/test*\")[1:]:\n",
    "        test_data_parts = np.vstack((test_data_parts, np.load(tt)))\n",
    "    test_data.append(copy.copy(test_data_parts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inception ResNet dimensions: (98304,)\n",
      "InceptionV3 dimensions: (131072,)\n",
      "Xception dimensions: (204800,)\n"
     ]
    }
   ],
   "source": [
    "for ind,name in enumerate(model_names):\n",
    "    print(\"\" + name + \" dimensions: \" + str(train_data[ind][0][0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_over_datasets(datasets):\n",
    "    ret_data = []\n",
    "    for data in datasets:\n",
    "        # rem here\n",
    "        n_data = split_input_target(data)\n",
    "        ret_data.append(n_data)\n",
    "    return ret_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target (data):\n",
    "    target = []\n",
    "    inp = []\n",
    "    for d in data:\n",
    "        target.append(d[1][0])\n",
    "        inp.append(d[0])\n",
    "    return [np.array(inp), target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp: only test purposes; remember to remove from iterate_over_datasets\n",
    "import random\n",
    "def split_input_target_special (data):\n",
    "    target = []\n",
    "    inp = []\n",
    "    for d in data:\n",
    "        target.append(random.randrange(0,2))\n",
    "        inp.append(d[0])\n",
    "    return [np.array(inp), target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = iterate_over_datasets(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = iterate_over_datasets(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reindex_classes(data):\n",
    "    for ind, ds in enumerate(data):\n",
    "        ds[1] = [x - min (ds[1]) for x in ds[1]]\n",
    "        data[ind] = ds\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = reindex_classes(train_data)\n",
    "test_data = reindex_classes(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data):\n",
    "    y_pred = model.predict(data[0])\n",
    "    \n",
    "    try: y_proba = model.predict_proba(data[0])\n",
    "    except: y_proba = model.predict(data[0])\n",
    "    \n",
    "    try: predictions = [round(value) for value in y_pred]\n",
    "    except: predictions = [np.argmax(value) for value in y_pred]\n",
    "    \n",
    "    # evaluate predictions\n",
    "    d = {}\n",
    "    d[\"accuracy\"] = accuracy_score(data[1], predictions)\n",
    "    d[\"top-2-accuracy\"] = get_top_n_accuracy(data[1], y_proba, n=2)\n",
    "    d[\"top-5-accuracy\"] = get_top_n_accuracy(data[1], y_proba, n=5)\n",
    "    d[\"top-10-accuracy\"] = get_top_n_accuracy(data[1], y_proba, n=10)\n",
    "    d[\"top-20-accuracy\"] = get_top_n_accuracy(data[1], y_proba, n=20)\n",
    "    d[\"confusion matrix\"] = confusion_matrix(data[1], predictions)\n",
    "    d[\"precision\"] = precision_score(data[1], predictions, average='macro')\n",
    "    d[\"recall\"] = recall_score(data[1], predictions, average='macro')\n",
    "    d[\"f1-score\"] = f1_score(data[1], predictions, average='macro')\n",
    "    # d[\"roc-auc\"] = roc_auc_score(data[1], predictions, )\n",
    "    d[\"cohen's kappa\"] = cohen_kappa_score(data[1], predictions)\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_measures(measure, data_type=\"data\", measure_name=\"Accuracy\"):\n",
    "    return \"%s in %s: %.2f\" % (measure_name, data_type, measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_measures(evaluation, t):\n",
    "    for key in evaluation.keys():\n",
    "        if key!=\"confusion matrix\":\n",
    "            print (get_measures(evaluation[key], t, key))\n",
    "        else:\n",
    "            print(key)\n",
    "            print(evaluation[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(train_eval, test_eval, model_stats, model_key, model_name, output_file_name):\n",
    "    file_stamp = curr_path.split(\"/\")[-2]\n",
    "    train_size = str(len(train_data[0][0]))\n",
    "    test_size = str(len(test_data[0][0]))\n",
    "    dataset_dim = str(np.max(train_data[0][1] + test_data[0][1]) + 1)\n",
    "    with open(output_file_name + '_train.csv', 'a') as f:\n",
    "        print (output_file_name + '_train.csv')\n",
    "        f.write(\"\\n\" + model_name + ',' + file_stamp + ',' + train_size + ',' + dataset_dim + ',' + model_key + \",\")\n",
    "        f.write(','.join([str(train_eval[x]) for x in train_eval.keys() if x!=\"confusion matrix\"]) + \",\")\n",
    "        f.write(','.join([str(x) for x in model_stats[\"train\"].values()]))\n",
    "    \n",
    "    with open(output_file_name + '_test.csv', 'a') as f:\n",
    "        f.write(\"\\n\" + model_name + ',' + file_stamp + ',' + test_size + ',' + dataset_dim + ',' + model_key + \",\")\n",
    "        f.write(','.join([str(test_eval[x]) for x in test_eval.keys() if x!=\"confusion matrix\"]) + \",\")    \n",
    "        f.write(','.join([str(x) for x in model_stats[\"test\"].values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_files(ind,prefix):\n",
    "    path = \"352_Benchmark_Results/\" + dataset_name + \"/\"\n",
    "    output_file_name = path + prefix + \"_\" + str(ind)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    with open(output_file_name + '_train.csv', 'w') as f:\n",
    "        f.write('Dataset,Dataset Stamp,Dataset Size,# Dataset Classes,Model Architecture' + ',' + \"accuracy\" + ',' + \"top-2-accuracy\" + ',' + \"top-5-accuracy\" + ',' + \"top-10-accuracy\" + ',' + \"top-20-accuracy\" + ',' + \"precision\" + ',' + \"recall\" + ',' + \"f1-score\" + ',' + \"cohen's kappa\" + ',' + \"Mean Accuracy over cross validation\" + ',' + \"mean standard deviation over cross validation\" + ',' + \"number observations over cross validation\")\n",
    "    with open(output_file_name + '_test.csv', 'w') as f:\n",
    "        f.write('Dataset,Dataset Stamp,Dataset Size,# Dataset Classes,Model Architecture' + ',' + \"accuracy\" + ',' + \"top-2-accuracy\" + ',' + \"top-5-accuracy\" + ',' + \"top-10-accuracy\" + ',' + \"top-20-accuracy\" + ',' + \"precision\" + ',' + \"recall\" + ',' + \"f1-score\" + ',' + \"cohen's kappa\" + ',' + \"Mean Accuracy over cross validation\" + ',' + \"mean standard deviation over cross validation\" + ',' + \"number observations over cross validation\")\n",
    "    return output_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_sources (training_src, test_src, validation_src=\"\"):\n",
    "    train = get_data_from_source(training_src)\n",
    "    test = get_data_from_source(test_src)\n",
    "    try:\n",
    "        if validation_src !=\"\":\n",
    "            val = get_data_from_source(validation_src)\n",
    "        else:\n",
    "            inp_test, inp_val, target_test, target_val = train_test_split(*test)\n",
    "            test = (inp_test, target_test)\n",
    "            val = (inp_val, target_val)\n",
    "    except: \n",
    "        inp_test, inp_val, target_test, target_val = train_test_split(*test)\n",
    "        test = (inp_test, target_test)\n",
    "        val = (inp_val, target_val)\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_A (input_shape, num_classes, learning_rate = 0.0001):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1024, input_shape=input_shape))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adam(lr=learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_B (input_shape, num_classes, learning_rate = 0.0001):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape=input_shape))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adam(lr=learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_C (input_shape, num_classes, learning_rate = 0.0001):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_classes, input_shape=input_shape))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adam(lr=learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_models(input_shape, num_classes, learning_rate = 0.0001):\n",
    "    mlp_models = []\n",
    "    mlp_models.append(create_model_A(input_shape, num_classes, learning_rate))\n",
    "    mlp_models.append(create_model_B(input_shape, num_classes, learning_rate))\n",
    "    mlp_models.append(create_model_C(input_shape, num_classes, learning_rate))\n",
    "    return mlp_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP hyperparameters\n",
    "epochs=[10]\n",
    "learning_rate= [0.001, 0.0001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP-0_LR-0.001\n",
      "MLP-1_LR-0.001\n",
      "MLP-2_LR-0.001\n",
      "MLP-0_LR-0.0001\n",
      "MLP-1_LR-0.0001\n",
      "MLP-2_LR-0.0001\n",
      "MLP-0_LR-0.001\n",
      "MLP-1_LR-0.001\n",
      "MLP-2_LR-0.001\n",
      "MLP-0_LR-0.0001\n",
      "MLP-1_LR-0.0001\n",
      "MLP-2_LR-0.0001\n",
      "MLP-0_LR-0.001\n",
      "MLP-1_LR-0.001\n",
      "MLP-2_LR-0.001\n",
      "MLP-0_LR-0.0001\n",
      "MLP-1_LR-0.0001\n",
      "MLP-2_LR-0.0001\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "mlp_models = []\n",
    "for i in learning_rate:\n",
    "    for j in range(3):\n",
    "        mlp_models.append({})\n",
    "num_classes = len(set(train_data[0][1]))\n",
    "mlp_names = []\n",
    "\n",
    "for tr_ind, tr_data in enumerate(train_data):\n",
    "    t_ind = copy.deepcopy(tr_ind)\n",
    "    input_shape = tr_data[0][0].shape\n",
    "    for lr_ind, lr in enumerate(learning_rate):\n",
    "        temp_models = create_models(input_shape, num_classes, lr)\n",
    "        # for mi, mm in enumerate(mlp_models):\n",
    "        for mi in range (len(temp_models)):\n",
    "            ind = copy.deepcopy(mi)\n",
    "            mlp_models[lr_ind * len(temp_models) + ind][model_names[t_ind]] = temp_models[ind]\n",
    "            model_name = \"MLP-{}_LR-{}\".format(mi,lr)\n",
    "            mlp_names.append(model_name)\n",
    "            print (model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Inception ResNet': <keras.engine.sequential.Sequential at 0x10775f160>,\n",
       "  'InceptionV3': <keras.engine.sequential.Sequential at 0x1a97aed8d0>,\n",
       "  'Xception': <keras.engine.sequential.Sequential at 0x1a17498c50>},\n",
       " {'Inception ResNet': <keras.engine.sequential.Sequential at 0x107743fd0>,\n",
       "  'InceptionV3': <keras.engine.sequential.Sequential at 0x1a98a70cf8>,\n",
       "  'Xception': <keras.engine.sequential.Sequential at 0x1a1754e278>},\n",
       " {'Inception ResNet': <keras.engine.sequential.Sequential at 0x1a1729f4e0>,\n",
       "  'InceptionV3': <keras.engine.sequential.Sequential at 0x1a99348be0>,\n",
       "  'Xception': <keras.engine.sequential.Sequential at 0x1a176cdef0>},\n",
       " {'Inception ResNet': <keras.engine.sequential.Sequential at 0x1a66575b38>,\n",
       "  'InceptionV3': <keras.engine.sequential.Sequential at 0x1a172d8cc0>,\n",
       "  'Xception': <keras.engine.sequential.Sequential at 0x1a17769be0>},\n",
       " {'Inception ResNet': <keras.engine.sequential.Sequential at 0x1a95235908>,\n",
       "  'InceptionV3': <keras.engine.sequential.Sequential at 0x1a17384ac8>,\n",
       "  'Xception': <keras.engine.sequential.Sequential at 0x1a1782b080>},\n",
       " {'Inception ResNet': <keras.engine.sequential.Sequential at 0x1a96913f60>,\n",
       "  'InceptionV3': <keras.engine.sequential.Sequential at 0x1a17455ac8>,\n",
       "  'Xception': <keras.engine.sequential.Sequential at 0x1a178fccc0>}]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_candidates = [\n",
    "  {'C': [1, 10], 'gamma': [0.001, 0.0001], 'kernel': ['rbf'], 'probability':[True]},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe_candidates = [\n",
    "  {'n_estimators': [100, 1000], 'criterion': ['gini', \"entropy\"]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "adb_candidates = [\n",
    "  {'n_estimators': [10, 100, 1000], 'learning_rate': [1.0, 0.5, 0.1]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc_candidates = [\n",
    "  {'n_estimators': [100], 'learning_rate': [0.1]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_candidates= [\n",
    "    {'penalty': [\"l1\", \"l2\"], 'C': [0.001,0.01,0.1,1,10,100]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_candidates= [\n",
    "    {'max_depth':[5,10,50], 'min_samples_split':np.linspace(0.1, 0.8, 8, endpoint=True), 'min_samples_leaf':np.linspace(0.1, 0.5, 5, endpoint=True), 'max_features':[10,100,500]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbc_candidates = [\n",
    "    {\n",
    "        'max_depth':[1,5,10], 'learning_rate':[0.01, 0.1], 'n_estimators':[10, 100]     # objective, booster\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_candidates = [\n",
    "    {\n",
    "        'n_neighbors':[1,10],\n",
    "#        'leaf_size':[1,5],\n",
    "    }  \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_candidates = {}\n",
    "parameter_candidates [\"svm\"] = svm_candidates\n",
    "parameter_candidates[\"rfe\"] = rfe_candidates\n",
    "parameter_candidates [\"adb\"] = adb_candidates\n",
    "parameter_candidates[\"gbc\"] = gbc_candidates\n",
    "parameter_candidates[\"lr\"] = lr_candidates\n",
    "parameter_candidates [\"dtc\"] = dtc_candidates\n",
    "parameter_candidates[\"xgbc\"] = xgbc_candidates\n",
    "parameter_candidates[\"knn\"] = knn_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mlp_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe = RandomForestClassifier()\n",
    "adb = AdaBoostClassifier()\n",
    "gbc = GradientBoostingClassifier()\n",
    "lr = LogisticRegression()\n",
    "dtc = DecisionTreeClassifier()#\n",
    "xgbc = XGBClassifier()\n",
    "svc = svm.SVC()\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "benchmark_models = {\"rfe\":rfe, \"xgbc\":xgbc, \"adb\":adb, \"gbc\":gbc, \"lr\":lr, \"dtc\":dtc, \"svm\":svc, \"knn\":knn}\n",
    "\n",
    "for x in range(len(mlp_models)):\n",
    "    benchmark_models[mlp_names[x]] = mlp_models[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False),\n",
       " 'svm': SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "   decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "   max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "   tol=0.001, verbose=False),\n",
       " 'knn': KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "            metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "            weights='uniform')}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_stats(cv_results, best_params, folds):     \n",
    "    truth = [param == best_params for param in cv_results['params']]\n",
    "            \n",
    "    curr_model_stats = {}\n",
    "    curr_model_stats[\"train\"] = {}\n",
    "    curr_model_stats[\"train\"][\"mean\"] = cv_results['mean_train_score'][truth][0]\n",
    "    curr_model_stats[\"train\"][\"std_dev\"] = cv_results['std_train_score'][truth][0]\n",
    "    curr_model_stats[\"train\"][\"observations\"] = folds\n",
    "    curr_model_stats[\"test\"] = {}\n",
    "    curr_model_stats[\"test\"][\"mean\"] = cv_results['mean_test_score'][truth][0]\n",
    "    curr_model_stats[\"test\"][\"std_dev\"] = cv_results['std_test_score'][truth][0]\n",
    "    curr_model_stats[\"test\"][\"observations\"] = folds\n",
    "    print(\"Mean: {}\".format(cv_results['mean_test_score'][truth]))\n",
    "    print(\"Standard deviation: {}\".format(cv_results['std_test_score'][truth]))\n",
    "    print(\"Number of observations: {}\".format(folds))\n",
    "    return curr_model_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_stats(hist):\n",
    "    plt.plot(hist.history['acc'])\n",
    "    plt.plot(hist.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(hist.history['loss'])\n",
    "    plt.plot(hist.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_model(benchmark_models, parameter_candidates, train, test,model_name, output_file_name, folds = 4, prefix = \"00000000\"):\n",
    "    model_stats = {}\n",
    "    #try:\n",
    "    for model_key in benchmark_models.keys():\n",
    "        model = benchmark_models[model_key]\n",
    "        print ((\" Model: \" + str(model_key)+ \" \").center(30, '#'))\n",
    "        start_time = time.time()\n",
    "        # try:\n",
    "        try:\n",
    "            clf = GridSearchCV(estimator=model, param_grid=parameter_candidates[model_key], n_jobs=4, cv=folds,return_train_score=True)\n",
    "            clf.fit(*train)\n",
    "            model = clf.best_estimator_\n",
    "            print (\"Best parameters: {}\".format(clf.best_params_))\n",
    "            path = \"352_Benchmark_Results/\" + prefix + \"_\" + dataset_name + \"/\" + model_name + \"/\"\n",
    "            print(path)\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "            filename = path + model_key + \".sav\"\n",
    "\n",
    "            try:\n",
    "                pickle.dump(clf, open(filename, 'wb'))\n",
    "            except:\n",
    "                print(filename + \" could not be created.\")\n",
    "            curr_model_stats = get_model_stats(clf.cv_results_, clf.best_params_, folds)\n",
    "        except:\n",
    "            model = benchmark_models[model_key][model_name]\n",
    "            kf = KFold(n_splits=folds)\n",
    "            cv_results = {}\n",
    "            target_cat = np.array(to_categorical(train[1]))\n",
    "            # print (target_cat.shape)\n",
    "            cv_results[\"train\"] = []\n",
    "            cv_results[\"test\"] = []\n",
    "            curr_model_stats = {\n",
    "                    \"train\": {\n",
    "                        \"mean\":0,\n",
    "                        \"std_dev\":0,\n",
    "                        \"observations\":0\n",
    "                    },\n",
    "                    \"test\": {\n",
    "                        \"mean\":0,\n",
    "                        \"std_dev\":0,\n",
    "                        \"observations\":0\n",
    "                    }\n",
    "                }\n",
    "            Wsave = model.get_weights()\n",
    "            for epoch in epochs:\n",
    "                for kfold_ind, indices in enumerate(kf.split(train[0])):\n",
    "                    model.set_weights(Wsave)\n",
    "                    train_index, test_index = indices\n",
    "                    X_train, X_test = train[0][train_index], train[0][test_index]\n",
    "                    y_train, y_test = target_cat[train_index], target_cat[test_index]\n",
    "                    # print(y_train)\n",
    "                    # tmp_model = keras.models.clone_model(model)\n",
    "                    # tmp_model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                    #   optimizer=keras.optimizers.Adam(lr=learning_rate),\n",
    "                    #   metrics=['accuracy'])\n",
    "                    history = model.fit(X_train, y_train, epochs = epoch, validation_data = (X_test, y_test))\n",
    "                    path = \"352_Benchmark_Results/\" + prefix + \"_\" + dataset_name + \"/\" + model_name + \"/\"\n",
    "                    if not os.path.exists(path):\n",
    "                        os.makedirs(path)\n",
    "                    model.save_weights(path + str(kfold_ind) + \"_\" + model_key + \".h5\")\n",
    "\n",
    "                    print(model_key)\n",
    "                    plot_model_stats(history)\n",
    "                    train_res = model.evaluate(X_train, y=y_train)[1]\n",
    "                    test_res = model.evaluate(X_test, y=y_test)[1]\n",
    "                    cv_results[\"train\"].append(train_res)\n",
    "                    cv_results[\"test\"].append(test_res)\n",
    "                # benchmark_models[model_key] = model\n",
    "                if curr_model_stats[\"test\"][\"mean\"] < statistics.mean(cv_results[\"test\"]):\n",
    "                    curr_model_stats = {\n",
    "                        \"train\": {\n",
    "                            \"mean\":statistics.mean(cv_results[\"train\"]),\n",
    "                            \"std_dev\":statistics.stdev(cv_results[\"train\"]),\n",
    "                            \"observations\":len(cv_results[\"train\"])\n",
    "                        },\n",
    "                        \"test\": {\n",
    "                            \"mean\":statistics.mean(cv_results[\"test\"]),\n",
    "                            \"std_dev\":statistics.stdev(cv_results[\"test\"]),\n",
    "                            \"observations\":len(cv_results[\"test\"])\n",
    "                        }\n",
    "                    }\n",
    "        model_stats[model_key] = curr_model_stats           \n",
    "\n",
    "\n",
    "        train_eval = evaluate(model, train)\n",
    "        test_eval = evaluate(model, test)\n",
    "        print_measures(train_eval, \"Train\")\n",
    "        print_measures(test_eval, \"Test\")\n",
    "        print(\"Total Time: {}\".format(time.time() - start_time))\n",
    "        write_to_file(train_eval, test_eval, curr_model_stats, model_key, model_name, output_file_name)\n",
    "\n",
    "        # benchmark_models[model_key][model_name] = model\n",
    "        # except:\n",
    "        #    print(model_key)\n",
    "    # except:\n",
    "    #     print(\"Failed but not discontinued\")\n",
    "    #    pass\n",
    "    return benchmark_models, model_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_data with shape (n,m)\n",
    "def get_top_n_accuracy(input_target, input_data, n=5):\n",
    "    count = 0\n",
    "    inp_data = input_data.copy()\n",
    "    for ind, pred in enumerate(inp_data):\n",
    "        max_classes = []\n",
    "        for i in range (n):\n",
    "            max_classes.append(np.argmax(pred))\n",
    "            pred[np.argmax(pred)]=-1\n",
    "        if input_target[ind] in max_classes: \n",
    "            count += 1\n",
    "    return count/len(input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Inception ResNet': <keras.engine.sequential.Sequential at 0x10775f160>,\n",
       "  'InceptionV3': <keras.engine.sequential.Sequential at 0x1a97aed8d0>,\n",
       "  'Xception': <keras.engine.sequential.Sequential at 0x1a17498c50>},\n",
       " {'Inception ResNet': <keras.engine.sequential.Sequential at 0x107743fd0>,\n",
       "  'InceptionV3': <keras.engine.sequential.Sequential at 0x1a98a70cf8>,\n",
       "  'Xception': <keras.engine.sequential.Sequential at 0x1a1754e278>},\n",
       " {'Inception ResNet': <keras.engine.sequential.Sequential at 0x1a1729f4e0>,\n",
       "  'InceptionV3': <keras.engine.sequential.Sequential at 0x1a99348be0>,\n",
       "  'Xception': <keras.engine.sequential.Sequential at 0x1a176cdef0>},\n",
       " {'Inception ResNet': <keras.engine.sequential.Sequential at 0x1a66575b38>,\n",
       "  'InceptionV3': <keras.engine.sequential.Sequential at 0x1a172d8cc0>,\n",
       "  'Xception': <keras.engine.sequential.Sequential at 0x1a17769be0>},\n",
       " {'Inception ResNet': <keras.engine.sequential.Sequential at 0x1a95235908>,\n",
       "  'InceptionV3': <keras.engine.sequential.Sequential at 0x1a17384ac8>,\n",
       "  'Xception': <keras.engine.sequential.Sequential at 0x1a1782b080>},\n",
       " {'Inception ResNet': <keras.engine.sequential.Sequential at 0x1a96913f60>,\n",
       "  'InceptionV3': <keras.engine.sequential.Sequential at 0x1a17455ac8>,\n",
       "  'Xception': <keras.engine.sequential.Sequential at 0x1a178fccc0>}]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m,d in enumerate(mlp_models):\n",
    "    for key, value in d.items():\n",
    "        model_json = value.to_json()\n",
    "        with open(\"352_Benchmark_Results/temp/{}_model_{}.json\".format(key,m), \"w\") as json_file:\n",
    "            json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________\n",
      "############################################################\n",
      "##################### Dataset Index: 0 #####################\n",
      "############################################################\n",
      "######### Model: lr ##########\n",
      "Best parameters: {'C': 0.001, 'penalty': 'l2'}\n",
      "352_Benchmark_Results/20181029_1157_ILSVRC2012/InceptionV3/\n",
      "Mean: [0.84586667]\n",
      "Standard deviation: [0.00275314]\n",
      "Number of observations: 2\n",
      "accuracy in Train: 1.00\n",
      "top-2-accuracy in Train: 1.00\n",
      "top-5-accuracy in Train: 1.00\n",
      "top-10-accuracy in Train: 1.00\n",
      "top-20-accuracy in Train: 1.00\n",
      "confusion matrix\n",
      "[[34  0  0 ...  0  0  0]\n",
      " [ 0 34  0 ...  0  0  0]\n",
      " [ 0  0 41 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 37  0  0]\n",
      " [ 0  0  0 ...  0 36  0]\n",
      " [ 0  0  0 ...  0  0 35]]\n",
      "precision in Train: 1.00\n",
      "recall in Train: 1.00\n",
      "f1-score in Train: 1.00\n",
      "cohen's kappa in Train: 1.00\n",
      "accuracy in Test: 0.85\n",
      "top-2-accuracy in Test: 0.93\n",
      "top-5-accuracy in Test: 0.98\n",
      "top-10-accuracy in Test: 0.99\n",
      "top-20-accuracy in Test: 0.99\n",
      "confusion matrix\n",
      "[[ 7  0  0 ...  0  0  0]\n",
      " [ 0 14  0 ...  0  0  0]\n",
      " [ 0  0  9 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 13  0  0]\n",
      " [ 0  0  0 ...  0 13  0]\n",
      " [ 0  0  0 ...  0  0 15]]\n",
      "precision in Test: 0.85\n",
      "recall in Test: 0.86\n",
      "f1-score in Test: 0.85\n",
      "cohen's kappa in Test: 0.85\n",
      "Total Time: 13416.819394111633\n",
      "352_Benchmark_Results/ILSVRC2012/20181029_1157_0_train.csv\n",
      "######### Model: svm #########\n",
      "Best parameters: {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf', 'probability': True}\n",
      "352_Benchmark_Results/20181029_1157_ILSVRC2012/InceptionV3/\n",
      "352_Benchmark_Results/20181029_1157_ILSVRC2012/InceptionV3/svm.sav could not be created.\n",
      "Mean: [0.3168]\n",
      "Standard deviation: [0.01256322]\n",
      "Number of observations: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathanjanke/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/jonathanjanke/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy in Train: 1.00\n",
      "top-2-accuracy in Train: 0.79\n",
      "top-5-accuracy in Train: 0.80\n",
      "top-10-accuracy in Train: 0.80\n",
      "top-20-accuracy in Train: 0.81\n",
      "confusion matrix\n",
      "[[34  0  0 ...  0  0  0]\n",
      " [ 0 34  0 ...  0  0  0]\n",
      " [ 0  0 41 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 37  0  0]\n",
      " [ 0  0  0 ...  0 35  0]\n",
      " [ 0  0  0 ...  0  0 35]]\n",
      "precision in Train: 1.00\n",
      "recall in Train: 1.00\n",
      "f1-score in Train: 1.00\n",
      "cohen's kappa in Train: 1.00\n",
      "accuracy in Test: 0.36\n",
      "top-2-accuracy in Test: 0.62\n",
      "top-5-accuracy in Test: 0.66\n",
      "top-10-accuracy in Test: 0.70\n",
      "top-20-accuracy in Test: 0.76\n",
      "confusion matrix\n",
      "[[1 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 5 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 8 0 0]\n",
      " [0 0 0 ... 0 7 0]\n",
      " [0 0 0 ... 0 0 8]]\n",
      "precision in Test: 0.79\n",
      "recall in Test: 0.39\n",
      "f1-score in Test: 0.46\n",
      "cohen's kappa in Test: 0.35\n",
      "Total Time: 50271.630492925644\n",
      "352_Benchmark_Results/ILSVRC2012/20181029_1157_0_train.csv\n",
      "######### Model: knn #########\n",
      "Best parameters: {'n_neighbors': 1}\n",
      "352_Benchmark_Results/20181029_1157_ILSVRC2012/InceptionV3/\n",
      "352_Benchmark_Results/20181029_1157_ILSVRC2012/InceptionV3/knn.sav could not be created.\n",
      "Mean: [0.75093333]\n",
      "Standard deviation: [0.00174695]\n",
      "Number of observations: 2\n",
      "accuracy in Train: 1.00\n",
      "top-2-accuracy in Train: 1.00\n",
      "top-5-accuracy in Train: 1.00\n",
      "top-10-accuracy in Train: 1.00\n",
      "top-20-accuracy in Train: 1.00\n",
      "confusion matrix\n",
      "[[34  0  0 ...  0  0  0]\n",
      " [ 0 34  0 ...  0  0  0]\n",
      " [ 0  0 41 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 37  0  0]\n",
      " [ 0  0  0 ...  0 36  0]\n",
      " [ 0  0  0 ...  0  0 35]]\n",
      "precision in Train: 1.00\n",
      "recall in Train: 1.00\n",
      "f1-score in Train: 1.00\n",
      "cohen's kappa in Train: 1.00\n",
      "accuracy in Test: 0.76\n",
      "top-2-accuracy in Test: 0.77\n",
      "top-5-accuracy in Test: 0.78\n",
      "top-10-accuracy in Test: 0.79\n",
      "top-20-accuracy in Test: 0.81\n",
      "confusion matrix\n",
      "[[ 7  0  0 ...  0  0  0]\n",
      " [ 0 13  0 ...  0  0  0]\n",
      " [ 0  0  6 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 13  0  0]\n",
      " [ 0  0  0 ...  0 13  0]\n",
      " [ 0  0  0 ...  0  0 15]]\n",
      "precision in Test: 0.77\n",
      "recall in Test: 0.77\n",
      "f1-score in Test: 0.76\n",
      "cohen's kappa in Test: 0.76\n",
      "Total Time: 3504.551560163498\n",
      "352_Benchmark_Results/ILSVRC2012/20181029_1157_0_train.csv\n",
      "____________________________________________________________\n",
      "############################################################\n",
      "##################### Dataset Index: 1 #####################\n",
      "############################################################\n",
      "######### Model: lr ##########\n",
      "Best parameters: {'C': 0.001, 'penalty': 'l2'}\n",
      "352_Benchmark_Results/20181029_1157_ILSVRC2012/Xception/\n",
      "Mean: [0.8528]\n",
      "Standard deviation: [0.0026806]\n",
      "Number of observations: 2\n",
      "accuracy in Train: 1.00\n",
      "top-2-accuracy in Train: 1.00\n",
      "top-5-accuracy in Train: 1.00\n",
      "top-10-accuracy in Train: 1.00\n",
      "top-20-accuracy in Train: 1.00\n",
      "confusion matrix\n",
      "[[33  0  0 ...  0  0  0]\n",
      " [ 0 34  0 ...  0  0  0]\n",
      " [ 0  0 41 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 37  0  0]\n",
      " [ 0  0  0 ...  0 36  0]\n",
      " [ 0  0  0 ...  0  0 35]]\n",
      "precision in Train: 1.00\n",
      "recall in Train: 1.00\n",
      "f1-score in Train: 1.00\n",
      "cohen's kappa in Train: 1.00\n",
      "accuracy in Test: 0.85\n",
      "top-2-accuracy in Test: 0.93\n",
      "top-5-accuracy in Test: 0.98\n",
      "top-10-accuracy in Test: 0.99\n",
      "top-20-accuracy in Test: 1.00\n",
      "confusion matrix\n",
      "[[ 8  0  0 ...  0  0  0]\n",
      " [ 0 15  0 ...  0  0  0]\n",
      " [ 0  0  8 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 13  0  0]\n",
      " [ 0  0  0 ...  0 13  0]\n",
      " [ 0  0  0 ...  0  0 14]]\n",
      "precision in Test: 0.85\n",
      "recall in Test: 0.85\n",
      "f1-score in Test: 0.85\n",
      "cohen's kappa in Test: 0.85\n",
      "Total Time: 11644.022418022156\n",
      "352_Benchmark_Results/ILSVRC2012/20181029_1157_1_train.csv\n",
      "######### Model: svm #########\n",
      "Best parameters: {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf', 'probability': True}\n",
      "352_Benchmark_Results/20181029_1157_ILSVRC2012/Xception/\n",
      "352_Benchmark_Results/20181029_1157_ILSVRC2012/Xception/svm.sav could not be created.\n",
      "Mean: [0.74826667]\n",
      "Standard deviation: [0.01237632]\n",
      "Number of observations: 2\n",
      "accuracy in Train: 1.00\n",
      "top-2-accuracy in Train: 1.00\n",
      "top-5-accuracy in Train: 1.00\n",
      "top-10-accuracy in Train: 1.00\n",
      "top-20-accuracy in Train: 1.00\n",
      "confusion matrix\n",
      "[[34  0  0 ...  0  0  0]\n",
      " [ 0 34  0 ...  0  0  0]\n",
      " [ 0  0 41 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 37  0  0]\n",
      " [ 0  0  0 ...  0 36  0]\n",
      " [ 0  0  0 ...  0  0 35]]\n",
      "precision in Train: 1.00\n",
      "recall in Train: 1.00\n",
      "f1-score in Train: 1.00\n",
      "cohen's kappa in Train: 1.00\n",
      "accuracy in Test: 0.76\n",
      "top-2-accuracy in Test: 0.88\n",
      "top-5-accuracy in Test: 0.92\n",
      "top-10-accuracy in Test: 0.95\n",
      "top-20-accuracy in Test: 0.97\n",
      "confusion matrix\n",
      "[[ 6  0  0 ...  0  0  0]\n",
      " [ 0 13  0 ...  0  0  0]\n",
      " [ 0  0  8 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 12  0  0]\n",
      " [ 0  0  0 ...  0 12  0]\n",
      " [ 0  0  0 ...  0  0 12]]\n",
      "precision in Test: 0.86\n",
      "recall in Test: 0.77\n",
      "f1-score in Test: 0.80\n",
      "cohen's kappa in Test: 0.76\n",
      "Total Time: 100159.51900601387\n",
      "352_Benchmark_Results/ILSVRC2012/20181029_1157_1_train.csv\n",
      "######### Model: knn #########\n",
      "Best parameters: {'n_neighbors': 10}\n",
      "352_Benchmark_Results/20181029_1157_ILSVRC2012/Xception/\n",
      "352_Benchmark_Results/20181029_1157_ILSVRC2012/Xception/knn.sav could not be created.\n",
      "Mean: [0.8184]\n",
      "Standard deviation: [0.00938593]\n",
      "Number of observations: 2\n",
      "accuracy in Train: 0.87\n",
      "top-2-accuracy in Train: 0.96\n",
      "top-5-accuracy in Train: 0.99\n",
      "top-10-accuracy in Train: 1.00\n",
      "top-20-accuracy in Train: 1.00\n",
      "confusion matrix\n",
      "[[26  0  0 ...  0  0  0]\n",
      " [ 0 27  0 ...  0  0  0]\n",
      " [ 0  0 35 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 32  0  0]\n",
      " [ 0  0  0 ...  0 30  0]\n",
      " [ 0  0  0 ...  0  0 35]]\n",
      "precision in Train: 0.87\n",
      "recall in Train: 0.87\n",
      "f1-score in Train: 0.86\n",
      "cohen's kappa in Train: 0.87\n",
      "accuracy in Test: 0.83\n",
      "top-2-accuracy in Test: 0.92\n",
      "top-5-accuracy in Test: 0.95\n",
      "top-10-accuracy in Test: 0.96\n",
      "top-20-accuracy in Test: 0.96\n",
      "confusion matrix\n",
      "[[ 8  0  0 ...  0  0  0]\n",
      " [ 0 12  0 ...  0  0  0]\n",
      " [ 0  0  7 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 13  0  0]\n",
      " [ 0  0  0 ...  0 13  0]\n",
      " [ 0  0  0 ...  0  0 14]]\n",
      "precision in Test: 0.83\n",
      "recall in Test: 0.83\n",
      "f1-score in Test: 0.82\n",
      "cohen's kappa in Test: 0.83\n",
      "Total Time: 15681.059370994568\n",
      "352_Benchmark_Results/ILSVRC2012/20181029_1157_1_train.csv\n"
     ]
    }
   ],
   "source": [
    "final_models = []\n",
    "stats = {}\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "prefix = now.strftime(\"%Y%m%d_%H%M\")\n",
    "\n",
    "for ind, _ in enumerate(train_data):\n",
    "    \n",
    "    print ((\"\").center(60, '_'))\n",
    "    print ((\"\").center(60, '#'))\n",
    "    print ((\" Dataset Index: \" + str(ind)+ \" \").center(60, '#'))\n",
    "    print ((\"\").center(60, '#'))\n",
    "    file_name = create_data_files(ind,prefix)\n",
    "    m,stat = grid_search_model(benchmark_models, parameter_candidates, train_data[ind], test_data[ind], model_names[ind], file_name, 2, prefix)\n",
    "    final_models.append(m)\n",
    "    stats[model_names[ind]] = stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expects a dictionary object with the structure dataset -> classification_algorithm -> train/test -> mean/std_dev/observations\n",
    "# returns a dictionary with each being (dataset:significance_matrix as pandas DF), e.g., sig_matrix_dict[\"InceptionV3\"]\n",
    "def get_signficance_matrix(stats, base=\"test\", sig_level=0.05, one_sided = False):\n",
    "    if one_sided:\n",
    "        sig_level /= 2\n",
    "    sig_matrix_dict = {}\n",
    "    for dataset in stats.keys():\n",
    "        sig_matrix_dict[dataset] = []\n",
    "        model_entries = stats[dataset].keys()\n",
    "        for model_key in stats[dataset].keys():\n",
    "            dim_B = []\n",
    "            for compare_model_key in stats[dataset].keys():\n",
    "                t_stats = ttest_ind_from_stats(stats[dataset][model_key][base][\"mean\"], stats[dataset][model_key][base][\"std_dev\"], stats[dataset][model_key][base][\"observations\"], stats[dataset][compare_model_key][base][\"mean\"], stats[dataset][compare_model_key][base][\"std_dev\"], stats[dataset][compare_model_key][base][\"observations\"])\n",
    "                if t_stats[1] < sig_level and t_stats[0] < 0.0:\n",
    "                    dim_B.append(True)\n",
    "                else:\n",
    "                    dim_B.append(False)\n",
    "            sig_matrix_dict[dataset].append(dim_B)\n",
    "        sig_matrix_dict[dataset] = pd.DataFrame(sig_matrix_dict[dataset],index=model_entries,columns=model_entries)\n",
    "    return sig_matrix_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_signficance_matrix(stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
