{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, cohen_kappa_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, KFold\n",
    "\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "\n",
    "from scipy import stats\n",
    "import glob\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_src=[]\n",
    "test_src = []\n",
    "validation_src = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = glob.glob(\"data/intermediate/*.txt\")\n",
    "for file in file_list:\n",
    "    f_type = file.split(\"_pred_\")[-1].split(\"_\")[0]\n",
    "    if f_type == \"train\":\n",
    "        training_src.append(file)\n",
    "    elif f_type == \"val\":\n",
    "        validation_src.append(file)\n",
    "    elif f_type == \"test\":\n",
    "        test_src.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(src):\n",
    "    with open(src, 'r') as myfile:\n",
    "        data=myfile.read().replace('\\n', '')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treat_data(data_string):\n",
    "    data_string = data_string.replace(\"]\", \"\")\n",
    "    data_string = data_string.replace(\" \", \"\")\n",
    "    data_split = data_string.split(\"[\")\n",
    "    data_split = [(d.split(\",\")) for d in data_split]\n",
    "    data_split_clean = [d[:-1] for d in data_split[:-1]]\n",
    "    data_split_clean.append(data_split[-1])\n",
    "    data_split_clean = [d for d in data_split_clean if d]\n",
    "    return data_split_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data):\n",
    "    inp = []\n",
    "    target = []\n",
    "    for d in data:\n",
    "        inp.append([float(x) for x in d[:-1]])\n",
    "        target.append(float(d[-1]))\n",
    "    inp = np.array(inp)\n",
    "    target = np.array(target)\n",
    "    return inp,target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_source (src):\n",
    "    data_string = read_data(src)\n",
    "    data_split = treat_data(data_string)\n",
    "    return split_data(data_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data):\n",
    "    y_pred = model.predict(data[0])\n",
    "    try: predictions = [round(value) for value in y_pred]\n",
    "    except: predictions = [np.argmax(value) for value in y_pred]\n",
    "    # evaluate predictions\n",
    "    d = {}\n",
    "    d[\"accuracy\"] = accuracy_score(data[1], predictions)\n",
    "    d[\"confusion matrix\"] = confusion_matrix(data[1], predictions)\n",
    "    d[\"precision\"] = precision_score(data[1], predictions, average='macro')\n",
    "    d[\"recall\"] = recall_score(data[1], predictions, average='macro')\n",
    "    d[\"f1-score\"] = f1_score(data[1], predictions, average='macro')\n",
    "    # d[\"roc-auc\"] = roc_auc_score(data[1], predictions, )\n",
    "    d[\"cohen's kappa\"] = cohen_kappa_score(data[1], predictions)\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_measures(measure, data_type=\"data\", measure_name=\"Accuracy\"):\n",
    "    return \"%s in %s: %.2f\" % (measure_name, data_type, measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_measures(evaluation, t):\n",
    "    for key in evaluation.keys():\n",
    "        if key!=\"confusion matrix\":\n",
    "            print (get_measures(evaluation[key], t, key))\n",
    "        else:\n",
    "            print(key)\n",
    "            print(evaluation[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_sources (training_src, test_src, validation_src=\"\"):\n",
    "    train = get_data_from_source(training_src)\n",
    "    test = get_data_from_source(test_src)\n",
    "    try:\n",
    "        if validation_src !=\"\":\n",
    "            val = get_data_from_source(validation_src)\n",
    "        else:\n",
    "            inp_test, inp_val, target_test, target_val = train_test_split(*test)\n",
    "            test = (inp_test, target_test)\n",
    "            val = (inp_val, target_val)\n",
    "    except: \n",
    "        inp_test, inp_val, target_test, target_val = train_test_split(*test)\n",
    "        test = (inp_test, target_test)\n",
    "        val = (inp_val, target_val)\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "test_data = []\n",
    "val_data = []\n",
    "\n",
    "ind = 0\n",
    "for ind, _ in enumerate(training_src):\n",
    "    if ind>=len(validation_src):\n",
    "        val_src = \"\"\n",
    "    else:\n",
    "        val_src = validation_src[ind]\n",
    "    train, val, test = get_data_from_sources(training_src[ind], test_src[ind], val_src)\n",
    "    train_data.append(train)\n",
    "    test_data.append(test)\n",
    "    val_data.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = int(max(train_data[0][1])+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1024, input_shape=(2048,)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(lr=0.0001),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "mlp_models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(2048,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(lr=0.0001),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "mlp_models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(num_classes, input_shape=(2048,)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(lr=0.0001),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "mlp_models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_candidates = [\n",
    "  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "  # {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe_candidates = [\n",
    "  {'n_estimators': [10, 100, 1000], 'criterion': ['gini', \"entropy\"]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "adb_candidates = [\n",
    "  {'n_estimators': [10, 100, 1000], 'learning_rate': [1.0, 0.5, 0.1], 'algorithm':[\"SAMME.R\", \"SAMME\"]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc_candidates = [\n",
    "  {'n_estimators': [10, 100, 1000], 'learning_rate': [0.05, 0.1, 0.5], 'criterion':[\"friedman_mse\", \"mae\"]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_candidates= [\n",
    "    {'penalty': [\"l2\", \"l5\"]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_candidates= [\n",
    "    {'criterion': [\"l1\", \"l2\"], 'splitter':['best']}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbc_candidates = [\n",
    "    {\n",
    "        'max_depth':[10], 'learning_rate':[0.1], 'n_estimators':[1000]     # objective, booster\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_candidates = {}\n",
    "parameter_candidates [\"svm\"] = svm_candidates\n",
    "parameter_candidates[\"rfe\"] = rfe_candidates\n",
    "parameter_candidates [\"adb\"] = adb_candidates\n",
    "parameter_candidates[\"gbc\"] = gbc_candidates\n",
    "parameter_candidates[\"lr\"] = lr_candidates\n",
    "parameter_candidates [\"dtc\"] = dtc_candidates\n",
    "parameter_candidates[\"xgbc\"] = xgbc_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe = RandomForestClassifier()\n",
    "adb = AdaBoostClassifier()\n",
    "gbc = GradientBoostingClassifier()\n",
    "lr = LogisticRegression()\n",
    "dtc = DecisionTreeClassifier()#\n",
    "xgbc = XGBClassifier()\n",
    "svc = svm.SVC()\n",
    "# Genetic Programming-based\n",
    "\n",
    "# benchmark_models = {\"rfe\":rfe, \"xgbc\":xgbc, \"adb\":adb, \"gbc\":gbc, \"lr\":lr, \"dtc\":dtc, \"svm\":svc}\n",
    "benchmark_models = {\"rfe\":rfe, \"lr\":lr, \"svm\":svc}\n",
    "for x in range(len(mlp_models)):\n",
    "    benchmark_models[\"mlp_{}\".format(x)] = mlp_models[x] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_model(benchmark_models, parameter_candidates, train, val, test, folds = 4):\n",
    "    cv_results = {}\n",
    "    for model_key in benchmark_models.keys():\n",
    "        model = benchmark_models[model_key]\n",
    "        print ((\" Model: \" + str(model_key)+ \" \").center(30, '#'))\n",
    "        try:\n",
    "            try:\n",
    "                clf = GridSearchCV(estimator=model, param_grid=parameter_candidates[model_key], n_jobs=-1, cv=folds)\n",
    "                clf.fit (*train)\n",
    "                model = clf.best_estimator_\n",
    "                cv_results[model_key] = clf.cv_results_\n",
    "            except ValueError:\n",
    "                    kf = KFold(n_splits=folds)\n",
    "                    cv_results[model_key] = []\n",
    "                    for train_index, test_index in kf.split(X,):\n",
    "                        X_train, X_test = train[0][train_index], train[0][test_index]\n",
    "                        y_train, y_test = train[1][train_index], train[1][test_index]\n",
    "                        history = model.fit(X_train, to_categorical(y_train), epochs = epochs, validation_data = (X_test, to_categorical(y_test)))\n",
    "                        train_res = model.evaluate(X_train, y=y_train)\n",
    "                        cv_results[model_key].append(train_res)\n",
    "                    benchmark_models[model_key] = model\n",
    "            train_eval = evaluate(model, train)\n",
    "            val_eval = evaluate(model, val)\n",
    "            test_eval = evaluate(model, test)\n",
    "            print_measures(train_eval, \"Train\")\n",
    "            print_measures(val_eval, \"Validation\")\n",
    "            print_measures(test_eval, \"Test\")  \n",
    "\n",
    "        except:\n",
    "            print(\"Problem with input shape\")\n",
    "            continue\n",
    "        \n",
    "        benchmark_models[model_key] = model\n",
    "    return benchmark_models, cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######### Model: rfe #########\n",
      "Problem with input shape\n",
      "######### Model: lr ##########\n",
      "Problem with input shape\n",
      "######### Model: svm #########\n",
      "Problem with input shape\n",
      "######## Model: mlp_0 ########\n",
      "Problem with input shape\n",
      "######## Model: mlp_1 ########\n",
      "Problem with input shape\n",
      "######## Model: mlp_2 ########\n",
      "Problem with input shape\n",
      "######### Model: rfe #########\n",
      "Problem with input shape\n",
      "######### Model: lr ##########\n",
      "Problem with input shape\n",
      "######### Model: svm #########\n",
      "Problem with input shape\n",
      "######## Model: mlp_0 ########\n",
      "Problem with input shape\n",
      "######## Model: mlp_1 ########\n",
      "Problem with input shape\n",
      "######## Model: mlp_2 ########\n",
      "Problem with input shape\n",
      "######### Model: rfe #########\n",
      "Problem with input shape\n",
      "######### Model: lr ##########\n",
      "Problem with input shape\n",
      "######### Model: svm #########\n",
      "Problem with input shape\n",
      "######## Model: mlp_0 ########\n",
      "Problem with input shape\n",
      "######## Model: mlp_1 ########\n",
      "Problem with input shape\n",
      "######## Model: mlp_2 ########\n",
      "Problem with input shape\n",
      "######### Model: rfe #########\n",
      "accuracy in Train: 1.00\n",
      "confusion matrix\n",
      "[[387   0   0   0   0   0   0   0   0   0]\n",
      " [  0 367   0   0   0   0   0   0   0   0]\n",
      " [  0   0 364   0   0   0   0   0   0   0]\n",
      " [  0   0   0 367   0   0   0   0   0   0]\n",
      " [  0   0   0   0 374   0   0   0   0   0]\n",
      " [  0   0   0   0   0 380   0   0   0   0]\n",
      " [  0   0   0   0   0   0 368   0   0   0]\n",
      " [  0   0   0   0   0   0   0 386   0   0]\n",
      " [  0   0   0   0   0   0   0   0 384   0]\n",
      " [  0   0   0   0   0   0   0   0   0 373]]\n",
      "precision in Train: 1.00\n",
      "recall in Train: 1.00\n",
      "f1-score in Train: 1.00\n",
      "cohen's kappa in Train: 1.00\n",
      "accuracy in Validation: 0.66\n",
      "confusion matrix\n",
      "[[17  2  0  0  0  0  0  0  0  0]\n",
      " [ 1 16  1  2  0  0  0  0  2  0]\n",
      " [ 3  2 12  0  0  1  1  0  2  2]\n",
      " [ 0  0  1 13  8  1  0  1  3  0]\n",
      " [ 0  0  2  3 18  1  1  1  3  0]\n",
      " [ 3  1  1  1  1 16  0  0  0  1]\n",
      " [ 0  0  0  1  2  0 13  4  0  0]\n",
      " [ 1  1  1  1  3  2  2 19  2  0]\n",
      " [ 1  1  0  0  0  0  1  0 21  0]\n",
      " [ 2  1  4  0  0  1  1  1  1 19]]\n",
      "precision in Validation: 0.66\n",
      "recall in Validation: 0.67\n",
      "f1-score in Validation: 0.66\n",
      "cohen's kappa in Validation: 0.62\n",
      "accuracy in Test: 0.69\n",
      "confusion matrix\n",
      "[[68  4  4  0  0  0  4  0  0  1]\n",
      " [ 6 61  2  0  2  0  1  0  6  0]\n",
      " [ 2  8 46  0  3 10  3  0  4  1]\n",
      " [ 0  2  4 39 17  4  2  1  3  1]\n",
      " [ 0  2  4 13 44  2  1  1  3  1]\n",
      " [ 5  3  2  0  4 53  2  0  5  2]\n",
      " [ 0  1  3  1  5  0 57 11  2  0]\n",
      " [ 1  0  2  2  1  0  7 46  7  2]\n",
      " [ 0  2  3  3  3  4  3  3 55  0]\n",
      " [ 2  1  4  5  2  6  3  1  1 45]]\n",
      "precision in Test: 0.69\n",
      "recall in Test: 0.68\n",
      "f1-score in Test: 0.68\n",
      "cohen's kappa in Test: 0.65\n",
      "######### Model: lr ##########\n",
      "Problem with input shape\n",
      "######### Model: svm #########\n",
      "accuracy in Train: 1.00\n",
      "confusion matrix\n",
      "[[387   0   0   0   0   0   0   0   0   0]\n",
      " [  0 367   0   0   0   0   0   0   0   0]\n",
      " [  0   0 364   0   0   0   0   0   0   0]\n",
      " [  0   0   0 367   0   0   0   0   0   0]\n",
      " [  0   0   0   0 374   0   0   0   0   0]\n",
      " [  0   0   0   0   0 380   0   0   0   0]\n",
      " [  0   0   0   0   0   0 368   0   0   0]\n",
      " [  0   0   0   0   0   0   0 386   0   0]\n",
      " [  0   0   0   0   0   0   0   0 384   0]\n",
      " [  0   0   0   0   0   0   0   0   0 373]]\n",
      "precision in Train: 1.00\n",
      "recall in Train: 1.00\n",
      "f1-score in Train: 1.00\n",
      "cohen's kappa in Train: 1.00\n",
      "accuracy in Validation: 0.66\n",
      "confusion matrix\n",
      "[[19  0  0  0  0  0  0  0  0  0]\n",
      " [ 1 17  1  2  0  1  0  0  0  0]\n",
      " [ 4  1 13  0  1  0  1  0  2  1]\n",
      " [ 0  0  1 14  8  1  0  2  1  0]\n",
      " [ 0  2  1  6 14  2  1  3  0  0]\n",
      " [ 1  1  1  1  3 14  0  0  1  2]\n",
      " [ 0  0  1  1  1  0 14  2  0  1]\n",
      " [ 1  1  1  0  3  2  5 18  1  0]\n",
      " [ 0  0  1  0  0  1  0  0 21  1]\n",
      " [ 1  0  3  0  0  1  1  3  0 21]]\n",
      "precision in Validation: 0.66\n",
      "recall in Validation: 0.68\n",
      "f1-score in Validation: 0.67\n",
      "cohen's kappa in Validation: 0.62\n",
      "accuracy in Test: 0.72\n",
      "confusion matrix\n",
      "[[69  1  6  0  0  1  3  0  1  0]\n",
      " [ 6 64  1  1  1  1  1  1  2  0]\n",
      " [ 2  2 49  1  3 11  2  1  1  5]\n",
      " [ 0  0  2 46 12  1  3  2  3  4]\n",
      " [ 0  1  4 10 44  3  3  4  2  0]\n",
      " [ 1  3  4  2  3 54  2  0  3  4]\n",
      " [ 0  1  6  2  5  0 56  8  2  0]\n",
      " [ 0  1  2  1  2  0  6 54  1  1]\n",
      " [ 0  1  5  4  6  1  3  2 54  0]\n",
      " [ 1  0  3  6  2  1  1  2  1 53]]\n",
      "precision in Test: 0.73\n",
      "recall in Test: 0.72\n",
      "f1-score in Test: 0.72\n",
      "cohen's kappa in Test: 0.69\n",
      "######## Model: mlp_0 ########\n",
      "Problem with input shape\n",
      "######## Model: mlp_1 ########\n",
      "Problem with input shape\n",
      "######## Model: mlp_2 ########\n",
      "Problem with input shape\n"
     ]
    }
   ],
   "source": [
    "final_models = []\n",
    "cv_results = []\n",
    "for ind, _ in enumerate(train_data):\n",
    "    \n",
    "    print ((\"\").center(60, '_'))\n",
    "    print ((\"\").center(60, '#'))\n",
    "    print ((\" Dataset Index: \" + str(ind)+ \" \").center(60, '#'))\n",
    "    print ((\"\").center(60, '#'))\n",
    "    \n",
    "    m,cv = grid_search_model(benchmark_models, parameter_candidates, train_data[ind], val_data[ind], test_data[ind])\n",
    "    final_models.append(m)\n",
    "    cv_results.append(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
