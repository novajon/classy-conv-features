{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, cohen_kappa_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, KFold\n",
    "\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "\n",
    "from scipy import stats\n",
    "import glob\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder_name= \"20180617_2043\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "curr_path = \"351_Data\\\\ILSVRC2012_Validation\\\\Intermediate\\\\\" + folder_name + \"\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_names = [fn.split(\"\\\\\")[-1] for fn in glob.glob(curr_path + \"*\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Inception ResNet', 'InceptionV3', 'Xception']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = []\n",
    "test_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for fn in model_names:\n",
    "    train_data.append(np.load(curr_path + str(fn) + \"\\\\train.npy\"))\n",
    "    test_data.append(np.load(curr_path + str(fn) + \"\\\\test.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8], dtype=int64)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_over_datasets(datasets):\n",
    "    ret_data = []\n",
    "    for data in datasets:\n",
    "        # rem here\n",
    "        n_data = split_input_target(data)\n",
    "        ret_data.append(n_data)\n",
    "    return ret_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_input_target (data):\n",
    "    target = []\n",
    "    inp = []\n",
    "    for d in data:\n",
    "        target.append(d[1][0])\n",
    "        inp.append(d[0])\n",
    "    return [np.array(inp), target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# temp: only test purposes; remember to remove from iterate_over_datasets\n",
    "import random\n",
    "def split_input_target_special (data):\n",
    "    target = []\n",
    "    inp = []\n",
    "    for d in data:\n",
    "        target.append(random.randrange(0,2))\n",
    "        inp.append(d[0])\n",
    "    return [np.array(inp), target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = iterate_over_datasets(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = iterate_over_datasets(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reindex_classes(data):\n",
    "    for ind, ds in enumerate(data):\n",
    "        ds[1] = [x - min (ds[1]) for x in ds[1]]\n",
    "        data[ind] = ds\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = reindex_classes(train_data)\n",
    "test_data = reindex_classes(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, data):\n",
    "    y_pred = model.predict(data[0])\n",
    "    try: predictions = [round(value) for value in y_pred]\n",
    "    except: predictions = [np.argmax(value) for value in y_pred]\n",
    "    # evaluate predictions\n",
    "    d = {}\n",
    "    d[\"accuracy\"] = accuracy_score(data[1], predictions)\n",
    "    d[\"confusion matrix\"] = confusion_matrix(data[1], predictions)\n",
    "    d[\"precision\"] = precision_score(data[1], predictions, average='macro')\n",
    "    d[\"recall\"] = recall_score(data[1], predictions, average='macro')\n",
    "    d[\"f1-score\"] = f1_score(data[1], predictions, average='macro')\n",
    "    # d[\"roc-auc\"] = roc_auc_score(data[1], predictions, )\n",
    "    d[\"cohen's kappa\"] = cohen_kappa_score(data[1], predictions)\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_measures(measure, data_type=\"data\", measure_name=\"Accuracy\"):\n",
    "    return \"%s in %s: %.2f\" % (measure_name, data_type, measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_measures(evaluation, t):\n",
    "    for key in evaluation.keys():\n",
    "        if key!=\"confusion matrix\":\n",
    "            print (get_measures(evaluation[key], t, key))\n",
    "        else:\n",
    "            print(key)\n",
    "            print(evaluation[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data_from_sources (training_src, test_src, validation_src=\"\"):\n",
    "    train = get_data_from_source(training_src)\n",
    "    test = get_data_from_source(test_src)\n",
    "    try:\n",
    "        if validation_src !=\"\":\n",
    "            val = get_data_from_source(validation_src)\n",
    "        else:\n",
    "            inp_test, inp_val, target_test, target_val = train_test_split(*test)\n",
    "            test = (inp_test, target_test)\n",
    "            val = (inp_val, target_val)\n",
    "    except: \n",
    "        inp_test, inp_val, target_test, target_val = train_test_split(*test)\n",
    "        test = (inp_test, target_test)\n",
    "        val = (inp_val, target_val)\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model_A (input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1024, input_shape=input_shape))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adam(lr=0.0001),\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model_B (input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape=input_shape))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adam(lr=0.0001),\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model_C (input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_classes, input_shape=input_shape))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adam(lr=0.0001),\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_models(input_shape, num_classes):\n",
    "    mlp_models = []\n",
    "    mlp_models.append(create_model_A(input_shape, num_classes))\n",
    "    mlp_models.append(create_model_B(input_shape, num_classes))\n",
    "    mlp_models.append(create_model_C(input_shape, num_classes))\n",
    "    return mlp_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp_models = [{}]*len(train_data)\n",
    "num_classes = len(set(train_data[0][1]))\n",
    "for tr_ind, tr_data in enumerate(train_data):\n",
    "    input_shape = tr_data[0][0].shape\n",
    "    temp_models = create_models(input_shape, num_classes)\n",
    "    for mi, mm in enumerate(mlp_models):\n",
    "        mm[model_names[tr_ind]] = temp_models[mi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm_candidates = [\n",
    "  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "  # {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfe_candidates = [\n",
    "  {'n_estimators': [10, 100, 1000], 'criterion': ['gini', \"entropy\"]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adb_candidates = [\n",
    "  {'n_estimators': [10, 100, 1000], 'learning_rate': [1.0, 0.5, 0.1], 'algorithm':[\"SAMME.R\", \"SAMME\"]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gbc_candidates = [\n",
    "  {'n_estimators': [10, 100, 1000], 'learning_rate': [0.05, 0.1, 0.5], 'criterion':[\"friedman_mse\", \"mae\"]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_candidates= [\n",
    "    {'penalty': [\"l1\", \"l2\"]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtc_candidates= [\n",
    "    {'criterion': [\"l1\", \"l2\"], 'splitter':['best']}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgbc_candidates = [\n",
    "    {\n",
    "        'max_depth':[10], 'learning_rate':[0.1], 'n_estimators':[1000]     # objective, booster\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameter_candidates = {}\n",
    "parameter_candidates [\"svm\"] = svm_candidates\n",
    "parameter_candidates[\"rfe\"] = rfe_candidates\n",
    "parameter_candidates [\"adb\"] = adb_candidates\n",
    "parameter_candidates[\"gbc\"] = gbc_candidates\n",
    "parameter_candidates[\"lr\"] = lr_candidates\n",
    "parameter_candidates [\"dtc\"] = dtc_candidates\n",
    "parameter_candidates[\"xgbc\"] = xgbc_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfe = RandomForestClassifier()\n",
    "adb = AdaBoostClassifier()\n",
    "gbc = GradientBoostingClassifier()\n",
    "lr = LogisticRegression()\n",
    "dtc = DecisionTreeClassifier()#\n",
    "xgbc = XGBClassifier()\n",
    "svc = svm.SVC()\n",
    "# Genetic Programming-based\n",
    "\n",
    "benchmark_models = {\"rfe\":rfe, \"xgbc\":xgbc, \"adb\":adb, \"gbc\":gbc, \"lr\":lr, \"dtc\":dtc, \"svm\":svc}\n",
    "# benchmark_models = {\"rfe\":rfe, \"lr\":lr, \"svm\":svc}\n",
    "# benchmark_models = {}\n",
    "for x in range(len(mlp_models)):\n",
    "    benchmark_models[\"mlp_{}\".format(x)] = mlp_models[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_model(benchmark_models, parameter_candidates, train, test,model_name, folds = 2):\n",
    "    cv_results = {}\n",
    "    for model_key in benchmark_models.keys():\n",
    "        model = benchmark_models[model_key]\n",
    "        print ((\" Model: \" + str(model_key)+ \" \").center(30, '#'))\n",
    "        try:\n",
    "            try:\n",
    "                clf = GridSearchCV(estimator=model, param_grid=parameter_candidates[model_key], n_jobs=-1, cv=folds)\n",
    "                clf.fit (*train)\n",
    "                model = clf.best_estimator_\n",
    "                cv_results[model_key] = clf.cv_results_\n",
    "            except:\n",
    "                    model = benchmark_models[model_key][model_name]\n",
    "                    kf = KFold(n_splits=folds)\n",
    "                    cv_results[model_key] = []\n",
    "                    target_cat = np.array(to_categorical(train[1]))\n",
    "                    # print (target_cat.shape)\n",
    "                    for train_index, test_index in kf.split(train[0],):\n",
    "                        X_train, X_test = train[0][train_index], train[0][test_index]\n",
    "                        y_train, y_test = target_cat[train_index], target_cat[test_index]\n",
    "                        # print(y_train)\n",
    "                        history = model.fit(X_train, y_train, epochs = epochs, validation_data = (X_test, y_test))\n",
    "                        train_res = model.evaluate(X_train, y=y_train)\n",
    "                        cv_results[model_key].append(train_res)\n",
    "                    # benchmark_models[model_key] = model\n",
    "            train_eval = evaluate(model, train)\n",
    "            test_eval = evaluate(model, test)\n",
    "            print_measures(train_eval, \"Train\")\n",
    "            print_measures(test_eval, \"Test\")  \n",
    "\n",
    "            benchmark_models[model_key][model_name] = model\n",
    "        except:\n",
    "            print(model_key)\n",
    "    return benchmark_models, cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________\n",
      "############################################################\n",
      "##################### Dataset Index: 0 #####################\n",
      "############################################################\n",
      "######### Model: rfe #########\n",
      "accuracy in Train: 1.00\n",
      "confusion matrix\n",
      "[[40  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 38  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 36  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 42  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 34  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 40  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 31  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 37  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 41  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 36]]\n",
      "precision in Train: 1.00\n",
      "recall in Train: 1.00\n",
      "f1-score in Train: 1.00\n",
      "cohen's kappa in Train: 1.00\n",
      "accuracy in Test: 0.94\n",
      "confusion matrix\n",
      "[[ 9  0  1  0  0  0  0  0  0  0]\n",
      " [ 0 12  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 13  1  0  0  0  0  0  0]\n",
      " [ 0  0  0  8  0  0  0  0  0  0]\n",
      " [ 0  3  0  0 13  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 10  0  0  0  0]\n",
      " [ 2  0  0  0  0  0 17  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 13  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  9  0]\n",
      " [ 0  0  0  0  0  0  0  1  0 13]]\n",
      "precision in Test: 0.94\n",
      "recall in Test: 0.95\n",
      "f1-score in Test: 0.94\n",
      "cohen's kappa in Test: 0.93\n",
      "rfe\n",
      "######## Model: xgbc #########\n",
      "accuracy in Train: 1.00\n",
      "confusion matrix\n",
      "[[40  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 38  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 36  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 42  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 34  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 40  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 31  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 37  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 41  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 36]]\n",
      "precision in Train: 1.00\n",
      "recall in Train: 1.00\n",
      "f1-score in Train: 1.00\n",
      "cohen's kappa in Train: 1.00\n",
      "accuracy in Test: 0.86\n",
      "confusion matrix\n",
      "[[ 9  0  0  0  0  0  1  0  0  0]\n",
      " [ 0 10  0  0  1  0  1  0  0  0]\n",
      " [ 0  0 12  0  0  0  1  0  1  0]\n",
      " [ 0  0  0  8  0  0  0  0  0  0]\n",
      " [ 0  3  0  1 11  0  0  1  0  0]\n",
      " [ 0  0  0  0  0 10  0  0  0  0]\n",
      " [ 1  2  0  0  0  0 16  0  0  0]\n",
      " [ 0  0  0  0  0  1  0 11  1  0]\n",
      " [ 0  0  0  0  0  0  0  0  9  0]\n",
      " [ 0  0  0  0  0  0  1  1  0 12]]\n",
      "precision in Test: 0.87\n",
      "recall in Test: 0.88\n",
      "f1-score in Test: 0.87\n",
      "cohen's kappa in Test: 0.85\n",
      "xgbc\n",
      "######### Model: adb #########\n"
     ]
    }
   ],
   "source": [
    "final_models = []\n",
    "cv_results = []\n",
    "for ind, _ in enumerate(train_data):\n",
    "    \n",
    "    print ((\"\").center(60, '_'))\n",
    "    print ((\"\").center(60, '#'))\n",
    "    print ((\" Dataset Index: \" + str(ind)+ \" \").center(60, '#'))\n",
    "    print ((\"\").center(60, '#'))\n",
    "    \n",
    "    m,cv = grid_search_model(benchmark_models, parameter_candidates, train_data[ind], test_data[ind], model_names[ind])\n",
    "    final_models.append(m)\n",
    "    cv_results.append(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
